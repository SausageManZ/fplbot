{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Welcome! This notebook takes you through the process I went through while creating models to predict fantasy league scores for a player. Let me start by describing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have data in the form X[0], X[1], X[2], ... , X[35], Y for each player in the premier league for 2013-14 and 2016-17 season. I have cleaned the data for some fields but I will skip describing that process for now. A single entry in dictionary form looks like this -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{\n",
    "    'X': {\n",
    "        'assists_per_match_played': 0.1111111111111111,\n",
    "        'avg_assists_form': 0.0,\n",
    "        'avg_bps_form': 20.0,\n",
    "        'avg_clean_sheets_form': 0.0,\n",
    "        'avg_goals_conceded_form': 2.0,\n",
    "        'avg_goals_scored_form': 0.3333333333333333,\n",
    "        'avg_minutes_form': 86.66666666666667,\n",
    "        'avg_net_transfers_form': 103.33333333333333,\n",
    "        'avg_points_form': 4.0,\n",
    "        'avg_red_cards_form': 0.0,\n",
    "        'avg_saves_form': 0.0,\n",
    "        'avg_yellow_cards_form': 0.0,\n",
    "        'bps_per_match_played': 10.777777777777779,\n",
    "        'clean_sheets_per_match_played': 0.07407407407407407,\n",
    "        'goals_conceded_per_match_played': 1.0,\n",
    "        'goals_scored_per_match_played': 0.037037037037037035,\n",
    "        'is_at_home': 1,\n",
    "        'last_season_points_per_minutes': 0.03913894324853229,\n",
    "        'minutes_per_match_played': 57.925925925925924,\n",
    "        'net_transfers_per_match_played': 409.3703703703704,\n",
    "        'opponent_goals_conceded_per_match': 1.4324324324324325,\n",
    "        'opponent_goals_scored_per_match': 1.5405405405405406,\n",
    "        'opponent_points_per_match': 1.6486486486486487,\n",
    "        'opponent_points_per_match_last_season': 1.8157894736842106,\n",
    "        'points_per_match_played': 2.0,\n",
    "        'price': 4.4,\n",
    "        'price_change_form': 0.0,\n",
    "        'red_cards_per_match_played': 0.037037037037037035,\n",
    "        'saves_per_match_played': 0.0,\n",
    "        'team_goals_conceded_per_match': 1.3783783783783783,\n",
    "        'team_goals_scored_per_match': 0.8918918918918919,\n",
    "        'team_points_per_match': 0.918918918918919,\n",
    "        'team_points_per_match_last_season': 0.9736842105263158,\n",
    "        'yellow_cards_per_match_played': 0.07407407407407407\n",
    "    },\n",
    "    'Y': {u'points_scored': 2.0}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A bit about earlier trials\n",
    "\n",
    "Before I get into the details of what worked, let me describe some initial attempts that didn't.\n",
    "\n",
    "The primary problem with our data is that it is highly imbalanced. Most players score low, with only a few instances of players who score above 5 pts.\n",
    "\n",
    "Also, since charateristics of each outfield position are different, I decided to train the models seperately for forwards, midfielders, defenders and goalkeepers.\n",
    "\n",
    "Things I tried before settling in on the solution -\n",
    "\n",
    "* Linear Regression - Simply wasn't able to give any meaningful predictions. I suspect this is because of high non-linearity and imbalance\n",
    "\n",
    "* Regression using neural network - Just couldn't get it to work, all predictions seemed to be either concentrated in the low points zone or were very random\n",
    "\n",
    "This is when I decided to convert the problem into a classification one. I experimented with multiple bin sizes and finally came to this categorisation -\n",
    "    a. points scored less than 5 - 'low'\n",
    "    b. between 5 - 8 - 'medium'\n",
    "    c. greater than 8 - 'high'\n",
    "For midfielders, this gives a ratio of 'low': 'medium': 'high' as 21.1: 1.53: 1.0\n",
    "\n",
    "It is already clear that our data is heavily skewed in favor of 'low' points. To test our model in such a case, accuracy is a bad metric to use. If I predict all outcomes to be 'low', I immediately get accuracy of 89%, but it quite meaningless in terms of prediction. We will use a confusion matrix to tune our model instead as it gives a much better picture of how our model is performing. So next I tried -\n",
    "\n",
    "* 3 layer neural network (3nn) - As expected, it breaks down and predicts all outcomes to be 'low'. We need to somehow offset the imbalance in our data.\n",
    "\n",
    "* 3nn with oversampling using SMOTE - One of the techniques to overcome imbalance is oversampling. Here we create interpolated copies of minority class data points to balance the dataset. SMOTE is one such algorithm to create these artificial data points. Unfortunately, this didn't work in our case (why needs to be still evaluated).\n",
    "\n",
    "* 3nn with undersampling - Contrary to oversampling, undersampling creates balance in dataset by removing datapoints from the majority class. This though has a clear disadvantage of losing out on information. This alone also didn't work well on the model.\n",
    "\n",
    "* 3nn with class weights - Another way to tackle imbalance is by assigning additional costs or weights to each class in the loss calculation. This forces the loss function to assign more importance to the minority classes. I started with weights as ratios of datapoints for each class and gradually iterated to a value which was giving better confusion matrix for our validation data. Finally, something that seemed to work!\n",
    "\n",
    "* 5nn with class weights - Deeper networks are often better at defining complex relationships to identify minority classes. I started increasing the number of hidden layers and found that a 5 layered fully connected network was working well for our problem.\n",
    "\n",
    "The 5nn with weights was converging to a solution, but what I realized was that depending on the learning rate, it would sometimes converge to a local minima of 'all low' solution, for which the cost was only slightly higher than our desired model. It struck me that it might be a good idea to undersample the data a bit to increase this difference in loss and stabilize it.\n",
    "\n",
    "This brings us to the model which I am currently using to predict results, a 5-layered neural network with optimized class weights and 40% undersampling for majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    df,\n",
    "    trial,\n",
    "):\n",
    "    # # shuffle dataframe rows\n",
    "    if trial:\n",
    "        frac = 0.25\n",
    "    else:\n",
    "        frac = 1\n",
    "    df = df.sample(frac=frac).reset_index(drop=True)\n",
    "    dataset = df.values\n",
    "    # # split into X and Y\n",
    "    num_of_features = dataset.shape[1] - 1\n",
    "    X = dataset[:, 0:num_of_features]\n",
    "    Y = dataset[:, num_of_features]\n",
    "\n",
    "    # # bin data into categories\n",
    "    bins = CLASS_BINS\n",
    "    bin_names = CLASSES\n",
    "    categories = pd.cut(Y, bins, labels=bin_names)\n",
    "    \n",
    "    # # one hot encode\n",
    "    Y = pd.get_dummies(categories).values\n",
    "\n",
    "    # # split into training, test and validation sets\n",
    "    num_of_samples = X.shape[0]\n",
    "    val_ratio = 0.2\n",
    "    test_ratio = 0.1\n",
    "    train_ratio = 1 - val_ratio - test_ratio\n",
    "    num_of_val_samples = int(val_ratio * num_of_samples)\n",
    "    num_of_train_samples = int(train_ratio * num_of_samples)\n",
    "\n",
    "    X_train = X[0:(num_of_train_samples + 1), :]\n",
    "    Y_train = Y[0:(num_of_train_samples + 1), :]\n",
    "    X_val = X[num_of_train_samples:(num_of_train_samples + num_of_val_samples + 1), :]\n",
    "    Y_val = Y[num_of_train_samples:(num_of_train_samples + num_of_val_samples + 1), :]\n",
    "    X_test = X[num_of_train_samples + num_of_val_samples:, :]\n",
    "    Y_test = Y[num_of_train_samples + num_of_val_samples:, :]\n",
    "\n",
    "    # # fix random seed for reproducibility\n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # # standardize data and store mean and scale to file\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    mean_array = scaler.mean_\n",
    "    scale_array = scaler.scale_\n",
    "    X_train_transformed = scaler.transform(X_train)\n",
    "    X_val_transformed = (X_val - mean_array) / (scale_array)\n",
    "    X_test_transformed = (X_test - mean_array) / (scale_array)\n",
    "\n",
    "    data_dict = {\n",
    "        'train_data': (X_train_transformed, Y_train),\n",
    "        'val_data': (X_val_transformed, Y_val),\n",
    "        'test_data': (X_test_transformed, Y_test),\n",
    "        'norm_arrays': (mean_array, scale_array),\n",
    "        'num_of_features': num_of_features,\n",
    "    }\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We randomly shuffle the data first.\n",
    "\n",
    "We first categorize Y values as 'low', 'mid' and 'high'. Then we apply a trick called one-hot encoding to deal with multiple classes. This converts the Y-array to m X 3 from m X 1, with each row consisting of a 3 elements, one for each class. So, an example with value 'low' would now be represented as [1 0 0]. Similarly 'mid' -> [0 1 0] and 'high' -> [0 0 1].\n",
    "\n",
    "Now we split our examples into training (70%), validation (20%) and test (10%) datasets.\n",
    "\n",
    "Next, we normalize our training X values using scikit-learn's StandardScaler. This brings all values for our features in the same range with mean for all values being 0. This usually helps any machine learning algorithm perform better.\n",
    "\n",
    "We store the mean and scale arrays for transforming validation, test as well as any future values we might need to predict for. It is important to normalize the training set separately instead of doing it together with validation data as we don't want any information from our validation set to leak to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_undersampling(X, Y, class_num=0, frac_removed=0.4):\n",
    "    # # print stats before undersampling\n",
    "    print('number of training samples before undersampling = %s' % X.shape[0])\n",
    "    print('Y counter before undersampling -')\n",
    "    print(get_class_counts(Y))\n",
    "\n",
    "    # # get indices of rows to be removed for majority class\n",
    "    y_0 = Y[:, class_num]\n",
    "    low_indices = np.where(y_0 == 1)[0]\n",
    "    n_removed = int(low_indices.shape[0] * frac_removed)\n",
    "\n",
    "    # # delected removed rows\n",
    "    removed = low_indices[0:n_removed]\n",
    "    Y = np.delete(Y, removed, axis=0)\n",
    "    X = np.delete(X, removed, axis=0)\n",
    "\n",
    "    # # print stats after undersampling\n",
    "    print('number of training samples after undersampling = %s' % X.shape[0])\n",
    "    print('Y counter after undersampling -')\n",
    "    print(get_class_counts(Y))\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly remove 40% examples of majority class ('low') from the training dataset to make the dataset more balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def five_layer_nn(num_of_features=0):\n",
    "    K.set_learning_phase(1)\n",
    "\n",
    "    # # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        num_of_features,\n",
    "        input_dim=num_of_features,\n",
    "        W_regularizer=l1l2(0.01),\n",
    "        init='normal',\n",
    "        activation='relu'\n",
    "    ))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(int(num_of_features * 1.5), init='normal', activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(num_of_features / 2, init='normal', activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(num_of_features / 4, init='normal', activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(len(CLASSES), init='normal', activation='softmax'))\n",
    "\n",
    "    # # define loss optimizer\n",
    "    adam = Adam(lr=0.0002)\n",
    "\n",
    "    # # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy', fbeta_custom])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a simple neural network with 3 hidden layers using Keras. We use l1l2 and dropout regularization, softmax activation for output layer (outputs probabilities per class) and cross-entropy loss function. Note that we have already defined a custom f-measure metric (fbeta_custom) with beta=1 to have a single score to judge our model.\n",
    "\n",
    "The learning rate lr needed to be iterated upon to get good convergence. Too small a value would sometimes make the training stuck in a local minima. Too large a value makes it difficult to get good convergance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to optimise the weights for each class to get a good confusion matrix. I started with the ratios of examples for each class (adjusted for undersampling) and iterated as per predictions on validation data. Finally I obtained the following weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define class weight to tackle skew\n",
    "CLASS_WEIGHT = {\n",
    "    'forward': {\n",
    "        0: 1.0,\n",
    "        1: 7.96 * 0.6 * 0.7,\n",
    "        2: 18.30 * 0.6 * 0.7 * 0.8,\n",
    "    },\n",
    "    'midfielder': {\n",
    "        0: 1.0,\n",
    "        1: 13.8 * 0.6 * 0.7,\n",
    "        2: 21.1 * 0.6 * 0.7 * 0.8,\n",
    "    },\n",
    "    'defender': {\n",
    "        0: 1.0,\n",
    "        1: 5.58 * 0.6 * 0.7,\n",
    "        2: 30.71 * 0.6 * 0.7 * 0.8,\n",
    "    },\n",
    "    'goalkeeper': {\n",
    "        0: 1.0,\n",
    "        1: 4.70 * 0.6 * 0.7 * 1.05,\n",
    "        2: 27.03 * 0.6 * 0.7 * 0.7,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have copied the complete code for prediction below. The main method is train (at the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l1l2\n",
    "from keras.metrics import fbeta_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE    # , ADASYN\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import errno\n",
    "\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(__file__)\n",
    "\n",
    "\n",
    "# define fbeta beta fn\n",
    "def fbeta_custom(x, y):\n",
    "    return fbeta_score(x, y, beta=1.0)\n",
    "\n",
    "\n",
    "# # to create dir if dir does not exist\n",
    "def create_filepath(filename):\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        except OSError as exc:  # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "\n",
    "def shuffle_X_Y(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "CLASSES = [\n",
    "    'low',\n",
    "    'mid',\n",
    "    'high',\n",
    "]\n",
    "\n",
    "CLASS_BINS = [\n",
    "    -10.0,\n",
    "    4.0,\n",
    "    8.0,\n",
    "    100.0,\n",
    "]\n",
    "\n",
    "# define class weight to tackle skew\n",
    "CLASS_WEIGHT = {\n",
    "    'forward': {\n",
    "        0: 1.0,\n",
    "        1: 7.96 * 0.6 * 0.7,\n",
    "        2: 18.30 * 0.6 * 0.7 * 0.8,\n",
    "    },\n",
    "    'midfielder': {\n",
    "        0: 1.0,\n",
    "        1: 13.8 * 0.6 * 0.7,\n",
    "        2: 21.1 * 0.6 * 0.7 * 0.8,\n",
    "    },\n",
    "    'defender': {\n",
    "        0: 1.0,\n",
    "        1: 5.58 * 0.6 * 0.7,\n",
    "        2: 30.71 * 0.6 * 0.7 * 0.8,\n",
    "    },\n",
    "    'goalkeeper': {\n",
    "        0: 1.0,\n",
    "        1: 4.70 * 0.6 * 0.7 * 1.05,\n",
    "        2: 27.03 * 0.6 * 0.7 * 0.7,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def get_class_counts(Y):\n",
    "    y_1d = np.empty((Y.shape[0], 1), dtype=np.object_)\n",
    "    i = 0\n",
    "    for y_entry in Y:\n",
    "        class_num = np.where(y_entry == 1)[0][0]\n",
    "        y_1d[i] = CLASSES[class_num]\n",
    "        i += 1\n",
    "\n",
    "    y_1d = y_1d.ravel()\n",
    "    unique, counts = np.unique(y_1d, return_counts=True)\n",
    "    return dict(zip(unique, counts))\n",
    "\n",
    "\n",
    "def apply_undersampling(X, Y, class_num=0, frac_removed=0.4):\n",
    "    # # print stats before undersampling\n",
    "    print('number of training samples before undersampling = %s' % X.shape[0])\n",
    "    print('Y counter before undersampling -')\n",
    "    print(get_class_counts(Y))\n",
    "\n",
    "    # # get indices of rows to be removed for majority class\n",
    "    y_0 = Y[:, class_num]\n",
    "    low_indices = np.where(y_0 == 1)[0]\n",
    "    n_removed = int(low_indices.shape[0] * frac_removed)\n",
    "\n",
    "    # # delected removed rows\n",
    "    removed = low_indices[0:n_removed]\n",
    "    Y = np.delete(Y, removed, axis=0)\n",
    "    X = np.delete(X, removed, axis=0)\n",
    "\n",
    "    # # print stats after undersampling\n",
    "    print('number of training samples after undersampling = %s' % X.shape[0])\n",
    "    print('Y counter after undersampling -')\n",
    "    print(get_class_counts(Y))\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def apply_smote(X, Y, ratio=1.0):\n",
    "    print('number of training samples before smote = %s' % X.shape[0])\n",
    "    sm = SMOTE(kind='regular', ratio=ratio)\n",
    "    # ada = ADASYN(ratio=ratio)\n",
    "\n",
    "    # convert y to 1d\n",
    "    y_1d = np.empty((Y.shape[0], 1), dtype=np.object_)\n",
    "    i = 0\n",
    "    for y_entry in Y:\n",
    "        class_num = np.where(y_entry == 1)[0][0]\n",
    "        y_1d[i] = CLASSES[class_num]\n",
    "        i += 1\n",
    "\n",
    "    y_1d = y_1d.ravel()\n",
    "    print(y_1d.shape)\n",
    "    print(y_1d)\n",
    "    print('Y counter before SMOTE -')\n",
    "    unique, counts = np.unique(y_1d, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    X, Y = sm.fit_sample(X, y_1d)\n",
    "    # X, Y = ada.fit_sample(X, y_1d)\n",
    "    print('Y counter after 1st SMOTE -')\n",
    "    print(Counter(Y))\n",
    "    X, Y = sm.fit_sample(X, Y)\n",
    "    print('Y counter after 2nd SMOTE -')\n",
    "    print(Counter(Y))\n",
    "\n",
    "    # one hot encode again\n",
    "    i = 0\n",
    "    y_temp = np.zeros(shape=(len(Y), len(CLASSES)))\n",
    "    for y_entry in Y:\n",
    "        class_num = CLASSES.index(y_entry)\n",
    "        y_temp[i][class_num] = 1\n",
    "        i = i + 1\n",
    "    Y = y_temp\n",
    "    print('number of training samples after smote = %s' % X.shape[0])\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# # method to create confusion matrix\n",
    "def get_confusion_matrix_one_hot(model_results, truth):\n",
    "    '''model_results and truth should be for one-hot format, i.e, have >= 2 columns,\n",
    "    where truth is 0/1, and max along each row of model_results is model result\n",
    "    '''\n",
    "    assert model_results.shape == truth.shape\n",
    "    num_outputs = truth.shape[1]\n",
    "    confusion_matrix = np.zeros((num_outputs, num_outputs), dtype=np.int32)\n",
    "    predictions = np.argmax(model_results, axis=1)\n",
    "    assert len(predictions) == truth.shape[0]\n",
    "\n",
    "    for actual_class in range(num_outputs):\n",
    "        idx_examples_this_class = truth[:, actual_class] == 1\n",
    "        prediction_for_this_class = predictions[idx_examples_this_class]\n",
    "        for predicted_class in range(num_outputs):\n",
    "            count = np.sum(prediction_for_this_class == predicted_class)\n",
    "            confusion_matrix[actual_class, predicted_class] = count\n",
    "    assert np.sum(confusion_matrix) == len(truth)\n",
    "    assert np.sum(confusion_matrix) == np.sum(truth)\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "# # define models\n",
    "def three_layer_nn(num_of_features=0):\n",
    "    # # create model\n",
    "    K.set_learning_phase(1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        num_of_features,\n",
    "        input_dim=num_of_features,\n",
    "        W_regularizer=l1l2(0.1),\n",
    "        init='normal',\n",
    "        activation='relu'\n",
    "    ))\n",
    "    # # add dropout to regularize\n",
    "    model.add(Dropout(0.35))\n",
    "    # # add hidden layer\n",
    "    model.add(Dense(num_of_features, init='normal', activation='relu'))\n",
    "    # # add output layer\n",
    "    model.add(Dense(len(CLASSES), init='normal', activation='softmax'))\n",
    "    # # define loss optimiser\n",
    "    adam = Adam(lr=0.0002)\n",
    "    # # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy', fbeta_custom])\n",
    "    return model\n",
    "\n",
    "\n",
    "def four_layer_nn(num_of_features=0):\n",
    "    # # create model\n",
    "    K.set_learning_phase(1)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        num_of_features,\n",
    "        input_dim=num_of_features,\n",
    "        W_regularizer=l1l2(0.1),\n",
    "        init='normal',\n",
    "        activation='relu'\n",
    "    ))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(int(num_of_features * 1.5), init='normal', activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(num_of_features / 4, init='normal', activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(len(CLASSES), init='normal', activation='softmax'))\n",
    "    # # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', fbeta_custom])\n",
    "    return model\n",
    "\n",
    "\n",
    "def five_layer_nn(num_of_features=0):\n",
    "    K.set_learning_phase(1)\n",
    "\n",
    "    # # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        num_of_features,\n",
    "        input_dim=num_of_features,\n",
    "        W_regularizer=l1l2(0.01),\n",
    "        init='normal',\n",
    "        activation='relu'\n",
    "    ))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(int(num_of_features * 1.5), init='normal', activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(num_of_features / 2, init='normal', activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(num_of_features / 4, init='normal', activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(len(CLASSES), init='normal', activation='softmax'))\n",
    "\n",
    "    # # define loss optimizer\n",
    "    adam = Adam(lr=0.0002)\n",
    "\n",
    "    # # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy', fbeta_custom])\n",
    "    return model\n",
    "\n",
    "\n",
    "model_dict = {\n",
    "    'three_layer_nn': three_layer_nn,\n",
    "    'four_layer_nn': four_layer_nn,\n",
    "    'five_layer_nn': five_layer_nn,\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    df,\n",
    "    trial,\n",
    "):\n",
    "    # # shuffle dataframe rows\n",
    "    if trial:\n",
    "        frac = 0.25\n",
    "    else:\n",
    "        frac = 1\n",
    "    df = df.sample(frac=frac).reset_index(drop=True)\n",
    "    dataset = df.values\n",
    "\n",
    "    # # split into X and Y\n",
    "    num_of_features = dataset.shape[1] - 1\n",
    "    X = dataset[:, 0:num_of_features]\n",
    "    Y = dataset[:, num_of_features]\n",
    "\n",
    "    # # bin data into categories\n",
    "    bins = CLASS_BINS\n",
    "    bin_names = CLASSES\n",
    "    categories = pd.cut(Y, bins, labels=bin_names)\n",
    "\n",
    "    # # one hot encode\n",
    "    Y = pd.get_dummies(categories).values\n",
    "\n",
    "    # # split into training, test and validation sets\n",
    "    num_of_samples = X.shape[0]\n",
    "    val_ratio = 0.2\n",
    "    test_ratio = 0.1\n",
    "    train_ratio = 1 - val_ratio - test_ratio\n",
    "    num_of_val_samples = int(val_ratio * num_of_samples)\n",
    "    num_of_train_samples = int(train_ratio * num_of_samples)\n",
    "\n",
    "    X_train = X[0:(num_of_train_samples + 1), :]\n",
    "    Y_train = Y[0:(num_of_train_samples + 1), :]\n",
    "    X_val = X[num_of_train_samples:(num_of_train_samples + num_of_val_samples + 1), :]\n",
    "    Y_val = Y[num_of_train_samples:(num_of_train_samples + num_of_val_samples + 1), :]\n",
    "    X_test = X[num_of_train_samples + num_of_val_samples:, :]\n",
    "    Y_test = Y[num_of_train_samples + num_of_val_samples:, :]\n",
    "\n",
    "    # # fix random seed for reproducibility\n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # # standardize data and store mean and scale to file\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    mean_array = scaler.mean_\n",
    "    scale_array = scaler.scale_\n",
    "    X_train_transformed = scaler.transform(X_train)\n",
    "    X_val_transformed = (X_val - mean_array) / (scale_array)\n",
    "    X_test_transformed = (X_test - mean_array) / (scale_array)\n",
    "\n",
    "    # # return data\n",
    "    data_dict = {\n",
    "        'train_data': (X_train_transformed, Y_train),\n",
    "        'val_data': (X_val_transformed, Y_val),\n",
    "        'test_data': (X_test_transformed, Y_test),\n",
    "        'norm_arrays': (mean_array, scale_array),\n",
    "        'num_of_features': num_of_features,\n",
    "    }\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def save_model_files(\n",
    "    model_name,\n",
    "    model,\n",
    "    position,\n",
    "    mean_array,\n",
    "    scale_array\n",
    "):\n",
    "    mean_list = [mean for mean in mean_array]\n",
    "    scale_list = [scale for scale in scale_array]\n",
    "    # # dump model files\n",
    "    model_filepath = os.path.join(SCRIPT_DIR, 'dumps/%s/keras_%ss/keras_%ss.json' % (model_name, position, position))\n",
    "    create_filepath(model_filepath)\n",
    "    weights_filepath = os.path.join(SCRIPT_DIR, 'dumps/%s/keras_%ss/weights.h5' % (model_name, position))\n",
    "    create_filepath(weights_filepath)\n",
    "    mean_filepath = os.path.join(SCRIPT_DIR, 'dumps/%s/keras_%ss/mean.json' % (model_name, position))\n",
    "    create_filepath(mean_filepath)\n",
    "    scale_filepath = os.path.join(SCRIPT_DIR, 'dumps/%s/keras_%ss/scale.json' % (model_name, position))\n",
    "    create_filepath(scale_filepath)\n",
    "\n",
    "    # model.save(model_filepath)\n",
    "    model.save_weights(weights_filepath)\n",
    "    model_json = model.to_json()\n",
    "\n",
    "    with open(model_filepath, \"w+\") as f:\n",
    "        f.write(model_json)\n",
    "    with open(mean_filepath, 'w+') as f:\n",
    "        f.write(json.dumps(mean_list))\n",
    "    with open(scale_filepath, 'w+') as f:\n",
    "        f.write(json.dumps(scale_list))\n",
    "\n",
    "\n",
    "def train(\n",
    "    position='forward',\n",
    "    model_name='three_layer_nn',\n",
    "    use_class_weights=False,\n",
    "    smote=False,\n",
    "    undersampling=False,\n",
    "    trial=False,\n",
    "):\n",
    "    \"\"\"Creates a network, trains it and dumps it on disk\n",
    "    \"\"\"\n",
    "\n",
    "    # # load dataset\n",
    "    data_path = os.path.join(SCRIPT_DIR, 'fpl_%ss.ssv' % (position))\n",
    "    df = pd.read_csv(data_path, delim_whitespace=True, header=None)\n",
    "\n",
    "    # # preprocess data\n",
    "    processed_data = preprocess(df=df, trial=trial)\n",
    "    X_train_transformed, Y_train = processed_data['train_data']\n",
    "    X_val_transformed, Y_val = processed_data['val_data']\n",
    "    X_test_transformed, Y_test = processed_data['test_data']\n",
    "    mean_array, scale_array = processed_data['norm_arrays']\n",
    "    num_of_features = processed_data['num_of_features']\n",
    "\n",
    "    X_train_resampled, Y_train_resampled = X_train_transformed, Y_train\n",
    "\n",
    "    smote_ratio = 1.0\n",
    "    if undersampling:\n",
    "        # # Apply random undersampling for dominant class\n",
    "        X_train_resampled, Y_train_resampled = apply_undersampling(X_train_resampled, Y_train_resampled)\n",
    "    if smote:\n",
    "        # # Apply regular SMOTE\n",
    "        smote_ratio = 0.5\n",
    "        X_train_resampled, Y_train_resampled = apply_smote(X_train_resampled, Y_train_resampled, ratio=smote_ratio)\n",
    "\n",
    "    # # evaluate model with standardized dataset\n",
    "    CLASS_WEIGHT_UNIFORM = {\n",
    "        0: 1.,\n",
    "        1: 1.0,\n",
    "        2: 1.0,\n",
    "    }\n",
    "    if use_class_weights:\n",
    "        class_weight_dict = CLASS_WEIGHT[position]\n",
    "    else:\n",
    "        class_weight_dict = CLASS_WEIGHT_UNIFORM\n",
    "    print('Class weights - ')\n",
    "    print(class_weight_dict)\n",
    "    model_fn = model_dict[model_name]\n",
    "    model = model_fn(num_of_features=num_of_features)\n",
    "    # print(Y_train_resampled[0:10, :])\n",
    "    history = model.fit(\n",
    "        X_train_resampled,\n",
    "        Y_train_resampled,\n",
    "        nb_epoch=50,\n",
    "        batch_size=10,\n",
    "        verbose=0,\n",
    "        # validation_split=0.2,\n",
    "        validation_data=(X_val_transformed, Y_val),\n",
    "        class_weight=class_weight_dict,\n",
    "    )\n",
    "    val_predictions_keras = model.predict(X_val_transformed)\n",
    "    test_predictions_keras = model.predict(X_test_transformed)\n",
    "    # print(test_predictions_keras[0:10])\n",
    "\n",
    "    # # generate and print confusion matrix for validation and test data\n",
    "    val_conf_matrix = get_confusion_matrix_one_hot(val_predictions_keras, Y_val)\n",
    "    print('validation confusion matrix - ')\n",
    "    print(val_conf_matrix)\n",
    "    conf_matrix = get_confusion_matrix_one_hot(test_predictions_keras, Y_test)\n",
    "    print('test confusion matrix - ')\n",
    "    print(conf_matrix)\n",
    "\n",
    "    if not trial:\n",
    "        model_name = model_fn.__name__\n",
    "        save_model_files(model_name, model, position, mean_array, scale_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's run it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to run the model for midfielders and see how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training samples before undersampling = 6785\n",
      "Y counter before undersampling -\n",
      "{'high': 298, 'low': 6048, 'mid': 439}\n",
      "number of training samples after undersampling = 4366\n",
      "Y counter after undersampling -\n",
      "{'high': 298, 'low': 3629, 'mid': 439}\n",
      "Class weights - \n",
      "{0: 1.0, 1: 5.795999999999999, 2: 7.089600000000001}\n",
      "validation confusion matrix - \n",
      "[[1417  172  154]\n",
      " [  68   25   34]\n",
      " [  25   14   30]]\n",
      "test confusion matrix - \n",
      "[[687  91  79]\n",
      " [ 30  14  31]\n",
      " [  9   6  23]]\n"
     ]
    }
   ],
   "source": [
    "from analysis.lib_1 import classifier\n",
    "classifier.train(position='midfielder', use_class_weights=True, model_name='five_layer_nn', undersampling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices shown above are of the following format - \n",
    "\n",
    "                    (predicted 'low)    (predicted 'mid')    (predicted 'high')\n",
    "    (actual 'low')      1417                172                  154\n",
    "    (actual 'mid')      68                  25                   34\n",
    "    (actual ''high')    25                  14                   30\n",
    "\n",
    "First thing to note here is that the confusion matrix ratios are similar for validation as well as test cases. This is good, it means our model works well on previously unseen data.\n",
    "\n",
    "Another interesting point is that our model is having a hard time predicting the 'mid' scores. I suspect that is because of their being similar likelihood of these cases being either 'low' or 'high'. I will look into this in the future to see if this can be improved.\n",
    "\n",
    "But the great thing about this matrix is that it show our model is pretty good at predicting 'low' and 'high' classes! We are off to a good start :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work in progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future scope of work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Try anomaly detection algorithms and see how they work to predict high scoring players\n",
    "\n",
    "* See if I can manually clean data better instead of using random undersampling\n",
    "\n",
    "* See if I can engineer the features better to enhance performance\n",
    "\n",
    "* Investigate why oversampling was not working"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
