{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Welcome! This notebook takes you through the process I went through while creating models to predict fantasy league scores for a player. Let me start by describing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have data in the form X[0], X[1], X[2], ... , X[35], Y for each player in the premier league for 2013-14 and 2016-17 season. I have cleaned the data for some fields but I will skip describing that process for now. A single entry in dictionary form looks like this -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{\n",
    "    'X': {\n",
    "        'assists_per_match_played': 0.1111111111111111,\n",
    "        'avg_assists_form': 0.0,\n",
    "        'avg_bps_form': 20.0,\n",
    "        'avg_clean_sheets_form': 0.0,\n",
    "        'avg_goals_conceded_form': 2.0,\n",
    "        'avg_goals_scored_form': 0.3333333333333333,\n",
    "        'avg_minutes_form': 86.66666666666667,\n",
    "        'avg_net_transfers_form': 103.33333333333333,\n",
    "        'avg_points_form': 4.0,\n",
    "        'avg_red_cards_form': 0.0,\n",
    "        'avg_saves_form': 0.0,\n",
    "        'avg_yellow_cards_form': 0.0,\n",
    "        'bps_per_match_played': 10.777777777777779,\n",
    "        'clean_sheets_per_match_played': 0.07407407407407407,\n",
    "        'goals_conceded_per_match_played': 1.0,\n",
    "        'goals_scored_per_match_played': 0.037037037037037035,\n",
    "        'is_at_home': 1,\n",
    "        'last_season_points_per_minutes': 0.03913894324853229,\n",
    "        'minutes_per_match_played': 57.925925925925924,\n",
    "        'net_transfers_per_match_played': 409.3703703703704,\n",
    "        'opponent_goals_conceded_per_match': 1.4324324324324325,\n",
    "        'opponent_goals_scored_per_match': 1.5405405405405406,\n",
    "        'opponent_points_per_match': 1.6486486486486487,\n",
    "        'opponent_points_per_match_last_season': 1.8157894736842106,\n",
    "        'points_per_match_played': 2.0,\n",
    "        'price': 4.4,\n",
    "        'price_change_form': 0.0,\n",
    "        'red_cards_per_match_played': 0.037037037037037035,\n",
    "        'saves_per_match_played': 0.0,\n",
    "        'team_goals_conceded_per_match': 1.3783783783783783,\n",
    "        'team_goals_scored_per_match': 0.8918918918918919,\n",
    "        'team_points_per_match': 0.918918918918919,\n",
    "        'team_points_per_match_last_season': 0.9736842105263158,\n",
    "        'yellow_cards_per_match_played': 0.07407407407407407\n",
    "    },\n",
    "    'Y': {u'points_scored': 2.0}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A bit about earlier trials\n",
    "\n",
    "Before I get into the details of what worked, let me describe some initial attempts that didn't.\n",
    "\n",
    "The primary problem with our data is that it is highly imbalanced. Most players score low, with only a few instances of players who score above 5 pts.\n",
    "\n",
    "Also, since charateristics of each outfield position are different, I decided to train the models seperately for forwards, midfielders, defenders and goalkeepers.\n",
    "\n",
    "Things I tried before settling in on the solution -\n",
    "\n",
    "    * Linear Regression - Simply wasn't able to give any meaningful predictions. I suspect this is because of high non-linearity and imbalance\n",
    "\n",
    "    * Regression using neural network - Just couldn't get it to work, all predictions seemed to be either concentrated in the low points zone or were very random\n",
    "\n",
    "This is when I decided to convert the problem into a classification one. I experimented with multiple bin sizes and finally came to this categorisation -\n",
    "    a. points scored less than 5 - 'low'\n",
    "    b. between 5 - 8 - 'medium'\n",
    "    c. greater than 8 - 'high'\n",
    "For midfielders, this gives a ratio of 'low': 'medium': 'high' as 21.1: 1.53: 1.0\n",
    "\n",
    "It is already clear that our data is heavily skewed in favor of 'low' points. To test our model in such a case, accuracy is a bad metric to use. If I predict all outcomes to be 'low', I immediately get accuracy of 89%, but it quite meaningless in terms of prediction. We will use a confusion matrix to tune our model instead as it gives a much better picture of how our model is performing. So next I tried -\n",
    "\n",
    "    * 3 layer neural network (3nn) - As expected, it breaks down and predicts all outcomes to be 'low'. We need to somehow offset the imbalance in our data.\n",
    "\n",
    "    * 3nn with oversampling using SMOTE - One of the techniques to overcome imbalance is oversampling. Here we create interpolated copies of minority class data points to balance the dataset. SMOTE is one such algorithm to create these artificial data points. Unfortunately, this didn't work in our case (why needs to be still evaluated).\n",
    "\n",
    "    * 3nn with undersampling - Contrary to oversampling, undersampling creates balance in dataset by removing datapoints from the majority class. This though has a clear disadvantage of losing out on information. This alone also didn't work well on the model.\n",
    "\n",
    "    * 3nn with class weights - Another way to tackle imbalance is by assigning additional costs or weights to each class in the loss calculation. This forces the loss function to assign more importance to the minority classes. I started with weights as ratios of datapoints for each class and gradually iterated to a value which was giving better confusion matrix for our validation data. Finally, something that seemed to work!\n",
    "\n",
    "    * 5nn with class weights - Deeper networks are often better at defining complex relationships to identify minority classes. I started increasing the number of hidden layers and found that a 5 layered fully connected network was working well for our problem.\n",
    "\n",
    "The 5nn with weights was converging to a solution, but what I realized was that depending on the learning rate, it would sometimes converge to a local minima of 'all low' solution, for which the cost was only slightly higher than our desired model. It struck me that it might be a good idea to undersample the data a bit to increase this difference in loss and stabilize it.\n",
    "\n",
    "This brings us to the model which I am currently using to predict results, a 5-layered neural network with optimized class weights and 40% undersampling for majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    df,\n",
    "    trial,\n",
    "):\n",
    "    # # shuffle dataframe rows\n",
    "    if trial:\n",
    "        frac = 0.25\n",
    "    else:\n",
    "        frac = 1\n",
    "    df = df.sample(frac=frac).reset_index(drop=True)\n",
    "    dataset = df.values\n",
    "    # # split into X and Y\n",
    "    num_of_features = dataset.shape[1] - 1\n",
    "    X = dataset[:, 0:num_of_features]\n",
    "    Y = dataset[:, num_of_features]\n",
    "\n",
    "    # # bin data into categories\n",
    "    bins = CLASS_BINS\n",
    "    bin_names = CLASSES\n",
    "    categories = pd.cut(Y, bins, labels=bin_names)\n",
    "    \n",
    "    # # one hot encode\n",
    "    Y = pd.get_dummies(categories).values\n",
    "\n",
    "    # # split into training, test and validation sets\n",
    "    num_of_samples = X.shape[0]\n",
    "    val_ratio = 0.2\n",
    "    test_ratio = 0.1\n",
    "    train_ratio = 1 - val_ratio - test_ratio\n",
    "    num_of_val_samples = int(val_ratio * num_of_samples)\n",
    "    num_of_train_samples = int(train_ratio * num_of_samples)\n",
    "\n",
    "    X_train = X[0:(num_of_train_samples + 1), :]\n",
    "    Y_train = Y[0:(num_of_train_samples + 1), :]\n",
    "    X_val = X[num_of_train_samples:(num_of_train_samples + num_of_val_samples + 1), :]\n",
    "    Y_val = Y[num_of_train_samples:(num_of_train_samples + num_of_val_samples + 1), :]\n",
    "    X_test = X[num_of_train_samples + num_of_val_samples:, :]\n",
    "    Y_test = Y[num_of_train_samples + num_of_val_samples:, :]\n",
    "\n",
    "    # # fix random seed for reproducibility\n",
    "    seed = 7\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # # standardize data and store mean and scale to file\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    mean_array = scaler.mean_\n",
    "    scale_array = scaler.scale_\n",
    "    X_train_transformed = scaler.transform(X_train)\n",
    "    X_val_transformed = (X_val - mean_array) / (scale_array)\n",
    "    X_test_transformed = (X_test - mean_array) / (scale_array)\n",
    "\n",
    "    data_dict = {\n",
    "        'train_data': (X_train_transformed, Y_train),\n",
    "        'val_data': (X_val_transformed, Y_val),\n",
    "        'test_data': (X_test_transformed, Y_test),\n",
    "        'norm_arrays': (mean_array, scale_array),\n",
    "        'num_of_features': num_of_features,\n",
    "    }\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We randomly shuffle the data first.\n",
    "\n",
    "We first categorize Y values as 'low', 'mid' and 'high'. Then we apply a trick called one-hot encoding to deal with multiple classes. This converts the Y-array to m X 3 from m X 1, with each row consisting of a 3 elements, one for each class. So, an example with value 'low' would now be represented as [1 0 0]. Similarly 'mid' -> [0 1 0] and 'high' -> [0 0 1].\n",
    "\n",
    "Now we split our examples into training (70%), validation (20%) and test (10%) datasets.\n",
    "\n",
    "Next, we normalize our training X values using scikit-learn's StandardScaler. This brings all values for our features in the same range with mean for all values being 0. This usually helps any machine learning algorithm perform better.\n",
    "\n",
    "We store the mean and scale arrays for transforming validation, test as well as any future values we might need to predict for. It is important to normalize the training set separately instead of doing it together with validation data as we don't want any information from our validation set to leak to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_undersampling(X, Y, class_num=0, frac_removed=0.4):\n",
    "    # # print stats before undersampling\n",
    "    print('number of training samples before undersampling = %s' % X.shape[0])\n",
    "    print('Y counter before undersampling -')\n",
    "    print(get_class_counts(Y))\n",
    "\n",
    "    # # get indices of rows to be removed for majority class\n",
    "    y_0 = Y[:, class_num]\n",
    "    low_indices = np.where(y_0 == 1)[0]\n",
    "    n_removed = int(low_indices.shape[0] * frac_removed)\n",
    "\n",
    "    # # delected removed rows\n",
    "    removed = low_indices[0:n_removed]\n",
    "    Y = np.delete(Y, removed, axis=0)\n",
    "    X = np.delete(X, removed, axis=0)\n",
    "\n",
    "    # # print stats after undersampling\n",
    "    print('number of training samples after undersampling = %s' % X.shape[0])\n",
    "    print('Y counter after undersampling -')\n",
    "    print(get_class_counts(Y))\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly remove 40% examples of majority class ('low') from the training dataset to make the dataset more balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def five_layer_nn(num_of_features=0):\n",
    "    K.set_learning_phase(1)\n",
    "\n",
    "    # # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(\n",
    "        num_of_features,\n",
    "        input_dim=num_of_features,\n",
    "        W_regularizer=l1l2(0.01),\n",
    "        init='normal',\n",
    "        activation='relu'\n",
    "    ))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(int(num_of_features * 1.5), init='normal', activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(num_of_features / 2, init='normal', activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(num_of_features / 4, init='normal', activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(len(CLASSES), init='normal', activation='softmax'))\n",
    "\n",
    "    # # define loss optimizer\n",
    "    adam = Adam(lr=0.0002)\n",
    "\n",
    "    # # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy', fbeta_custom])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a simple neural network with 3 hidden layers using Keras. We use l1l2 and dropout regularization, softmax activation for output layer (outputs probabilities per class) and cross-entropy loss function. Note that we have already defined a custom f-measure metric (fbeta_custom) with beta=1 to have a single score to judge our model.\n",
    "\n",
    "The learning rate lr needed to be iterated upon to get good convergence. Too small a value would sometimes make the training stuck in a local minima. Too large a value makes it difficult to get good convergance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weight definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to optimise the weights for each class to get a good confusion matrix. I started with the ratios of examples for each class (adjusted for undersampling) and iterated as per predictions on validation data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
